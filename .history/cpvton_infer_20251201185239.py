# coding=utf-8
"""
CP-VTON+ Inference Wrapper
ÙŠÙˆÙØ± ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø³ÙŠØ·Ø© Ù„ØªØ´ØºÙŠÙ„ CP-VTON+ Ø¹Ù„Ù‰ ØµÙˆØ± Ø´Ø®ØµÙŠØ© ÙˆÙ…Ù„Ø§Ø¨Ø³
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
import numpy as np
from PIL import Image
import os

from networks import GMM, UnetGenerator


class CPVTONPlusModel:
    """
    Wrapper class Ù„Ù€ CP-VTON+ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙŠ RunPod Serverless
    
    ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­Ù…ÙŠÙ„ GMM Ùˆ TOM models ÙˆÙŠÙˆÙØ± Ø¯Ø§Ù„Ø© try_on Ø¨Ø³ÙŠØ·Ø©
    """
    
    def __init__(
        self,
        gmm_checkpoint: str,
        tom_checkpoint: str,
        device: str = "cuda",
        fine_width: int = 192,
        fine_height: int = 256,
        radius: int = 5,
        grid_size: int = 5
    ):
        """
        ØªÙ‡ÙŠØ¦Ø© CP-VTON+ Model
        
        Args:
            gmm_checkpoint: Ù…Ø³Ø§Ø± checkpoint Ù„Ù€ GMM
            tom_checkpoint: Ù…Ø³Ø§Ø± checkpoint Ù„Ù€ TOM
            device: Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (cuda Ø£Ùˆ cpu)
            fine_width: Ø¹Ø±Ø¶ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
            fine_height: Ø§Ø±ØªÙØ§Ø¹ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
            radius: Ù†ØµÙ Ù‚Ø·Ø± Ù„Ù€ pose points
            grid_size: Ø­Ø¬Ù… Ø§Ù„Ø´Ø¨ÙƒØ© Ù„Ù€ GMM
        """
        self.device = torch.device(device)
        self.fine_width = fine_width
        self.fine_height = fine_height
        self.radius = radius
        self.grid_size = grid_size
        
        # ØªØ­Ø¶ÙŠØ± transforms
        self.transform = transforms.Compose([
            transforms.Resize((fine_height, fine_width)),
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
        ])
        
        # ØªØ­Ù…ÙŠÙ„ grid image Ù„Ù„Ù€ GMM
        self.grid_image = self._load_grid_image()
        
        # ØªØ­Ù…ÙŠÙ„ GMM Model
        print(f"[CPVTONPlus] ØªØ­Ù…ÙŠÙ„ GMM Ù…Ù† {gmm_checkpoint}")
        
        # Ø¥Ù†Ø´Ø§Ø¡ dummy opt object Ù„Ù„Ù€ GMM
        class DummyOpt:
            def __init__(self, w, h, r, g):
                self.fine_width = w
                self.fine_height = h
                self.radius = r
                self.grid_size = g
        
        opt = DummyOpt(fine_width, fine_height, radius, grid_size)
        self.gmm = GMM(opt)
        self._load_checkpoint(self.gmm, gmm_checkpoint)
        self.gmm.to(self.device)
        self.gmm.eval()
        
        # ØªØ­Ù…ÙŠÙ„ TOM Model
        print(f"[CPVTONPlus] ØªØ­Ù…ÙŠÙ„ TOM Ù…Ù† {tom_checkpoint}")
        self.tom = UnetGenerator(26, 4, 6, ngf=64, norm_layer=nn.InstanceNorm2d)
        self._load_checkpoint(self.tom, tom_checkpoint)
        self.tom.to(self.device)
        self.tom.eval()
        
        print("[CPVTONPlus] âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Models Ø¨Ù†Ø¬Ø§Ø­!")
    
    def _load_grid_image(self) -> torch.Tensor:
        """ØªØ­Ù…ÙŠÙ„ grid image Ù„Ù„Ù€ GMM"""
        grid_path = "grid.png"
        if not os.path.exists(grid_path):
            # Ø¥Ù†Ø´Ø§Ø¡ grid Ø¨Ø³ÙŠØ· Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯
            grid = np.zeros((256, 192, 3), dtype=np.uint8)
            grid[::20, :] = 255
            grid[:, ::20] = 255
            im_g = Image.fromarray(grid)
        else:
            im_g = Image.open(grid_path)
        
        im_g = self.transform(im_g)
        return im_g.unsqueeze(0)  # Add batch dimension
    
    def _load_checkpoint(self, model: nn.Module, checkpoint_path: str):
        """ØªØ­Ù…ÙŠÙ„ checkpoint Ù„Ù„Ù…ÙˆØ¯ÙŠÙ„"""
        if not os.path.exists(checkpoint_path):
            print(f"[ØªØ­Ø°ÙŠØ±] checkpoint ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {checkpoint_path}")
            print(f"[ØªØ­Ø°ÙŠØ±] Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… weights Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©!")
            return
        
        try:
            state_dict = torch.load(checkpoint_path, map_location=self.device)
            model.load_state_dict(state_dict)
        except Exception as e:
            print(f"[Ø®Ø·Ø£] ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ checkpoint: {e}")
            print(f"[ØªØ­Ø°ÙŠØ±] Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… weights Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©!")
    
    def _preprocess_images(
        self,
        person_img: Image.Image,
        cloth_img: Image.Image
    ) -> tuple:
        """
        Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ± Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ù„Ù„Ù…ÙˆØ¯ÙŠÙ„
        
        ÙÙŠ Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø¨Ø³Ø·ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„ØµÙˆØ±Ø© ÙƒØ§Ù…Ù„Ø© Ø¨Ø¯ÙˆÙ† segmentation
        """
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ RGB
        person_img = person_img.convert("RGB")
        cloth_img = cloth_img.convert("RGB")
        
        # ØªØ·Ø¨ÙŠÙ‚ transforms
        person_tensor = self.transform(person_img).unsqueeze(0).to(self.device)
        cloth_tensor = self.transform(cloth_img).unsqueeze(0).to(self.device)
        
        # Ø¥Ù†Ø´Ø§Ø¡ cloth mask Ø¨Ø³ÙŠØ· (Ø§ÙØªØ±Ø¶ Ø£Ù† Ø§Ù„Ù…Ù„Ø§Ø¨Ø³ ØªØ­ØªÙ„ Ù…Ø¹Ø¸Ù… Ø§Ù„ØµÙˆØ±Ø©)
        # ÙÙŠ productionØŒ ÙŠØ¬Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù… segmentation model
        cloth_mask = torch.ones(1, 1, self.fine_height, self.fine_width).to(self.device)
        
        # Ø¥Ù†Ø´Ø§Ø¡ agnostic representation Ø¨Ø³ÙŠØ·
        # ÙÙŠ productionØŒ ÙŠØ¬Ø¨ Ø­Ø³Ø§Ø¨ Ù‡Ø°Ø§ Ù…Ù† parsing mask
        agnostic = person_tensor.clone()
        
        return person_tensor, cloth_tensor, cloth_mask, agnostic
    
    @torch.no_grad()
    def try_on(
        self,
        person_img: Image.Image,
        cloth_img: Image.Image
    ) -> Image.Image:
        """
        ØªØ´ØºÙŠÙ„ virtual try-on
        
        Args:
            person_img: ØµÙˆØ±Ø© Ø§Ù„Ø´Ø®Øµ (PIL Image)
            cloth_img: ØµÙˆØ±Ø© Ø§Ù„Ù…Ù„Ø§Ø¨Ø³ (PIL Image)
            
        Returns:
            ØµÙˆØ±Ø© Ø§Ù„Ù†ØªÙŠØ¬Ø© (PIL Image)
        """
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±
        person_tensor, cloth_tensor, cloth_mask, agnostic = self._preprocess_images(
            person_img, cloth_img
        )
        
        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: GMM - ØªØ´ÙˆÙŠÙ‡ Ø§Ù„Ù…Ù„Ø§Ø¨Ø³ Ù„ØªÙ†Ø§Ø³Ø¨ Ø§Ù„Ø¬Ø³Ù…
        grid_image = self.grid_image.to(self.device)
        grid, theta = self.gmm(agnostic, cloth_mask)
        warped_cloth = F.grid_sample(cloth_tensor, grid, padding_mode='border', align_corners=True)
        warped_mask = F.grid_sample(cloth_mask, grid, padding_mode='zeros', align_corners=True)
        
        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: TOM - Ø¯Ù…Ø¬ Ø§Ù„Ù…Ù„Ø§Ø¨Ø³ Ø§Ù„Ù…Ø´ÙˆÙ‡Ø© Ù…Ø¹ Ø§Ù„Ø´Ø®Øµ
        tom_input = torch.cat([agnostic, warped_cloth, warped_mask], 1)
        outputs = self.tom(tom_input)
        
        # ÙØµÙ„ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª
        p_rendered, m_composite = torch.split(outputs, 3, 1)
        p_rendered = torch.tanh(p_rendered)
        m_composite = torch.sigmoid(m_composite)
        
        # Ø¯Ù…Ø¬ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        p_tryon = warped_cloth * m_composite + p_rendered * (1 - m_composite)
        
        # ØªØ­ÙˆÙŠÙ„ Tensor Ø¥Ù„Ù‰ PIL Image
        result = self._tensor_to_image(p_tryon)
        
        return result
    
    def _tensor_to_image(self, tensor: torch.Tensor) -> Image.Image:
        """
        ØªØ­ÙˆÙŠÙ„ PyTorch tensor Ø¥Ù„Ù‰ PIL Image
        
        Args:
            tensor: Tensor Ø¨ØµÙŠØºØ© (B, C, H, W) Ù…Ø¹ Ù‚ÙŠÙ… [-1, 1]
            
        Returns:
            PIL Image
        """
        # Ø¥Ø²Ø§Ù„Ø© batch dimension ÙˆØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ CPU
        img_tensor = tensor.squeeze(0).cpu()
        
        # ØªØ­ÙˆÙŠÙ„ Ù…Ù† [-1, 1] Ø¥Ù„Ù‰ [0, 1]
        img_tensor = (img_tensor + 1) / 2
        
        # Clamp Ø§Ù„Ù‚ÙŠÙ…
        img_tensor = torch.clamp(img_tensor, 0, 1)
        
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ numpy array
        img_array = img_tensor.permute(1, 2, 0).numpy()
        
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ [0, 255]
        img_array = (img_array * 255).astype(np.uint8)
        
        # Ø¥Ù†Ø´Ø§Ø¡ PIL Image
        return Image.fromarray(img_array, mode='RGB')


if __name__ == "__main__":
    """
    Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ø³ÙŠØ· Ù„Ù„Ù…ÙˆØ¯ÙŠÙ„
    """
    print("=" * 80)
    print("Ø§Ø®ØªØ¨Ø§Ø± CP-VTON+ Model")
    print("=" * 80)
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
    model = CPVTONPlusModel(
        gmm_checkpoint="checkpoints/GMM/gmm_final.pth",
        tom_checkpoint="checkpoints/TOM/tom_final.pth",
        device="cpu"  # Ø§Ø³ØªØ®Ø¯Ù… "cuda" Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªÙˆÙØ±
    )
    
    # Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ø§Ø®ØªØ¨Ø§Ø±
    person_img = Image.new("RGB", (192, 256), color=(200, 200, 200))
    cloth_img = Image.new("RGB", (192, 256), color=(100, 150, 200))
    
    # ØªØ´ØºÙŠÙ„ try-on
    print("\nğŸ¨ ØªØ´ØºÙŠÙ„ Virtual Try-On...")
    result = model.try_on(person_img, cloth_img)
    
    print(f"âœ… Ù†Ø¬Ø­! Ø­Ø¬Ù… Ø§Ù„Ù†ØªÙŠØ¬Ø©: {result.size}")
    print("=" * 80)
